# Weekly Commits for 08/19/19

## 8/19/19

### Unsupervised Activity Discovery
- Read more into ELMO, how they do sequential context-based embeddings  <done> [categories: UAD, research]
- Implement sampling mechanisms (temporal, hierarchical) <done> [categories: UAD, research]
- Figure out how the base codebase implements model checkpointing, actually run it. <done> [categories: UAD, research]
- Figure out / ask Serena if relative time prediction or ranking loss seems more reasonable for temporal embedding <done> [categories: UAD, research]
  * Even for comparison / ranking loss. Could do based on neighoring context, or do based on relative time. Relative time would just be like for the positive hierarchy example, get the middle time frame and say that this distance is closer than the negative example's relative time. Could also just try both.
- Implement ground truth label loading <done> [categories: UAD, research]
  * Need `load_obj`, `save_obj`, `load_gt`, `create_mapping` equivalents

## 8/20/19

## 8/21/19

### Research
- Implement correct video loading setup with dataloader, which should just serve the tensors? How to pass a custom class then through the dataset / batch loader?  <done> [categories: UAD, research]
- Also don't use the dataset class and dataloader for the segments? <done> [categories: UAD, research]
- Do more reading into how the actual data loader interacts with pytorch and gpu stuff <done> [categories: UAD, research]


## 8/22/19

### Research  
- Same as above, implement dataloader - look into how this should be done with an LSTM? Even pre-trained LSTM might seem okay / good. Try this first but then need to have the buffering done. First should just try to do the mean though. <done> [categories: research]


## 8/23/19

### Research  
- Figure out best practices for loading variable sized segments as batches into pytorch model <done> [categories: research]  
  * Should this be at batch-level? What counts as GPU loading. Is the actual model input of multiple datapoints just vectorizing and does this boost speed a lot?  
- Implement dataloader s.t. we sample segments on the fly during initialization. Then for each batch it's based on this <done> [categories: research, UAD, code]  
- Add feature to specify how many single-frame segments we want. <done> [categories: research, UAD, code]  
- Update model to take in multiple segments at a time (in the fwd method). Then for embedding call a second embedding-specific method <done> [categories: research, UAD, code]  
- Make sure the inputs are all padded?  

Currently storing features twice. Do we need to keep global features?  
* Seems like we might not. also don't really need to have global indexing. And keeping all features in a global features?   

## 8/24/19  

### Hack Lodge  

Work on a thing for visual data storytelling / framework for this thing.  
- Set up the react app and setup the general navigation / flow [] [categories: code, hack lodge]
- Figure out the actual data backend. Should just be time-indexed json file. [] [categories: code, hack lodge]  
- Implement the D3 stuff with regard to the particle flow [] [categories: code, hack lodge]  

### Research  
- Fix bugs regarding sampling single frame segments <done> [categories: code, UAD, research]  
- Add more lenient video segment subsampling <done> [categories: code, UAD, research] 
  * Add argument for end margin, if the extra segment would go above the video cutoff but below the margin, add this segment but keep it bounded by the actual video cutoff
- Get working batch loading until the point where we have to pad things <done> [categories: code, UAD, research]  
  * Can probably play around with the split sizes as well

## 8/25/19  

### Research  
- Figure out / sanity check if the current training I'm doing is reasonable wrt learning hyperbolic embeddings and not using RSGD? <done> [categories: code, research, UAD]  
* From Khurlkov et al. (Hyperbolic Image Embeddings), they found no significant improvement with using Riemannian optimization techniques (like RAdam) vs just modeling things with Euclidean parameters before projecting to hyperbolic space in the last output  
* Implement padding for multiple sequence length padding inputs for batch size  

Other settings (like categories and their colors) can be found and edited in config.json.
