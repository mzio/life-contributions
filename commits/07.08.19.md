# Weekly Commits for 07/08/19  

## 7/8/19  

### Research  

#### MNIST toy dataset  
- Run pre-trained model 
- Figure out how the pre-trained code actually takes in input images  
- Run the things I already had on the cluster again, but this time using the pretrained model    

##### Setting up MNIST pretrained  
Ran with the following:  
1. Install dependencies: `ml python/2.7.13 py-h5py/2.7.1_py27 protobuf/3.4.0 py-protobuf/3.6.1_py27 py-numpy/1.14.3_py27 viz py-matplotlib/2.2.2_py27 py-scipy/1.1.0_py27 cuda/10.0.130`    
2. Compile proto file with `protoc -I=./ --python_out=./ config.proto`  
3. Make sure to update the directories for where things checkpoint and save re: weights.  
4. Run model with  
```
python lstm_combo.py models/lstm_combo_1layer_mnist_pretrained.pbtxt datasets/bouncing_mnist.pbtxt datasets/bouncing_mnist_valid.pbtxt 0  
```  
  * Ran with 3000 steps primarily.  
  * Also odd that the weight paths initially for the pretrained are not local.  
5. Finally to display results, run:  
```
python display_results.py models/lstm_combo_1layer_mnist_pretrained.pbtxt datasets/bouncing_mnist_valid.pbtxt 0
```
  * Make sure to update weight directory in the pbtxt file to the correct thing.

Model saved at `/scratch/users/mzhang20/unsupervised-videos/models/lstm_combo_20190708141826.h5` 
* Commented out `data_file: "/home/users/mzhang20/moving_mnist/unsupervised-videos/bouncing_mnist_test.npy"` in `datasets/bouncing_mnist_valid.pbtxt` 

Last model ended up training up to 87000 steps. 

 

#### Readings    
- Read papers on hyperbolic attention <done> [categories: research]     
- Try to internalize BERT, ELMO, Transformer-XL, XLNet better <done> [categories: research]    

## 7/9/19

### Surgery Tracking  
How do VR headsets do hand-tracking? Like they must be doing some egocentric visual understanding and mapping in real time.   

## 7/10  

## 7/11  

## 7/12  

## 7/13  

## 7/14  

